#!/bin/bash

# Build ollama (CUDA) and run it with podman for a specific LLM model
# Requires podman, nvidia-container-toolkit, jq and python3

# Usage: ./ollam.sh [start|stop|clean|restart|pull|list|log]

SCRIPTDIR=$(dirname $0)
DATADIR=${OLLAMA_DATA_DIR:-$(realpath $SCRIPTDIR/../data)}
LIBDIR=$(realpath $SCRIPTDIR/../lib)

source $LIBDIR/utils.sh
source $LIBDIR/ollama.sh

function start_action {
    podman_network_setup private_gpt
    ollama_run
}

function stop_action {
    ollama_stop
}

function clean_action {
    ollama_clean
    podman_network_clean private_gpt
}

function restart_action {
    stop_action
    start_action
}

function pull_action {
    if [ -z $1 ]; then
        echo "Usage: $(basename $0) pull [model]"
        return 1
    fi

    ollama_create
    ollama_start
    ollama_pull $2
    ollama_stop
}

function list_action {
    ollama_list
}

function log_action {
    ollama_log $2
}

function help_action {
    echo "Usage: $(basename $0) [start|stop|clean|restart|pull|list|log]"
}

ACTION=$1

if [ -z $ACTION ]; then
    help_action
    exit 1
fi

validate_requirements
ollama_create_dirs

case $ACTION in
    pull)
        pull_action $2
        ;;
    list)
        list_action
        ;;
    log)
        log_action $2
        ;;
    start)
        start_action
        ;;
    stop)
        stop_action
        ;;
    clean)
        stop_action
        clean_action
        ;;
    restart)
        restart_action
        ;;
    *)
        help_action
        exit 1
        ;;
esac

exit 0